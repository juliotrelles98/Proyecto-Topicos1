{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57 images belonging to 5 classes.\n",
      "Found 12 images belonging to 5 classes.\n",
      "WARNING:tensorflow:From <ipython-input-1-4186f53110d7>:74: Model.fit_generator (from tensorflow.python.keras.engine.training_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - ETA: 0s - batch: 499.5000 - size: 28.5000 - loss: 9.0396 - accuracy: 0.4384WARNING:tensorflow:From D:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "1000/1000 [==============================] - 529s 529ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0396 - accuracy: 0.4384 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 572s 572ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0471 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 583s 583ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0460 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 577s 577ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0477 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 521s 521ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0456 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 533s 533ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0471 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 545s 545ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0508 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 595s 595ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0512 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 559s 559ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0536 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 569s 569ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0512 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 565s 565ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0391 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 563s 563ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0523 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 559s 559ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0447 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 586s 586ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0473 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 552s 552ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0464 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 542s 542ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0447 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 569s 569ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0504 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 561s 561ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0487 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 565s 565ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0521 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 567s 567ms/step - batch: 499.5000 - size: 28.5000 - loss: 9.0528 - accuracy: 0.4386 - val_loss: 9.4022 - val_accuracy: 0.4167\n"
     ]
    }
   ],
   "source": [
    "import sys  ##librerias para moverse en el sistema operativo\n",
    "import os   ##librerias para moverse en el sistema operativo\n",
    "import tensorflow  as tf  \n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator ## preprocesar imagenes\n",
    "from tensorflow.python.keras import optimizers ##optimizadora para entrear nuestro CNN\n",
    "from tensorflow.python.keras.models import Sequential  ##libreria de redes secuenciales \n",
    "from tensorflow.python.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.python.keras.layers import  Convolution2D, MaxPooling2D\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "data_entrenamiento = './data/entrenamiento'\n",
    "data_validacion = './data/validacion'\n",
    "\n",
    "\"\"\"\n",
    "Parametros de la RED NEURONAL \n",
    "\"\"\"\n",
    "epocas=20 ## numero de veces iteracion sobre el set de entrenamiento\n",
    "longitud, altura = 150, 150 ##cambio de tama単o de imagenes\n",
    "batch_size = 32 ##Numero de imagenes para procesar en cada uno de los pasos\n",
    "pasos = 1000 ## numero de veces que se va procesar la informacion\n",
    "pasos_validacion = 200 ## ver su esta aprendiendo nuestra cnn\n",
    "filtrosConv1 = 32 ## profundidad de imagen de 32\n",
    "filtrosConv2 = 64 ## profundida de imagen de 64\n",
    "tamano_filtro1 = (3, 3)  ## tama単o de filtro\n",
    "tamano_filtro2 = (2, 2)   ## tama単o de filtro\n",
    "tamano_pool = (2, 2) ## tama単o de filtro de Maxpooling\n",
    "clases = 5 ## cantidad de clase de frutas que aprendera \n",
    "lr = 0.0005 ## ratio de aprendimiento\n",
    "\n",
    "\n",
    "##pre procesamiento de imagenes de nuestra CNN\n",
    "\n",
    "entrenamiento_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255, ## re escaldo de imagenes el rango de los pixeles\n",
    "    shear_range=0.2,  ## inclinar las imagenes para mejor aprendizaje\n",
    "    zoom_range=0.2,   ## zoom a algunas imagenes \n",
    "    horizontal_flip=True) ## invertir la imagen\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255) ## re escaldo de imagenes el rango de los pixeles\n",
    "\n",
    "entrenamiento_generador = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validacion_generador = test_datagen.flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "##RED CNN\n",
    "\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(longitud, altura, 3), activation='relu'))  ## primera capa de convolucion\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\")) ## segunda capa de convolucion\n",
    "cnn.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "cnn.add(Flatten())## contiene todo la info de la cnn en una sola dimension\n",
    "cnn.add(Dense(256, activation='relu')) ## neuronas conectadas con la capa flatten\n",
    "cnn.add(Dropout(0.5))## evitar sobreajuste apagando el 50% de la neuronas\n",
    "cnn.add(Dense(clases, activation='softmax'))  ## softmax = es probabilidad\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',optimizer=optimizers.Adam(lr=lr),metrics=['accuracy']) ##optimizacion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cnn.fit_generator(entrenamiento_generador, steps_per_epoch=pasos,epochs=epocas, validation_data=validacion_generador, validation_steps=pasos_validacion)\n",
    "\n",
    "target_dir = './modelo/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "cnn.save('./modelo/modelo.h5') ## save de estructura de modelo\n",
    "cnn.save_weights('./modelo/pesos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
